# Revisiting Input-Label Demonstrations: In-Context Learning with Large Language Models

**Abstract:** In-context learning enables large language models to adapt to new tasks by conditioning on demonstrations, without parameter updates. Prior work suggests that correct input-label pairings in demonstrations are not essential for strong performance. We systematically re-evaluate these claims using recent models, including GPT-3.5-turbo, GPT-4.1-mini, Qwen2.5, and Llama-3.2. We also extend beyond prior work by evaluating more complex math reasoning tasks, which have been less studied in the context of input-label demonstration learning. For multiple-choice and classification tasks, we find that label correctness continues to have minimal impact on performance in newer chat models. Similar trends emerge in math reasoning tasks; in fact, the models sometimes perform better when shown incorrect solutions. Overall, our results indicate a reduced sensitivity to in-context learning signals in the latest generation of models.

**Acknowledgements:** We would like to express our gratitude to our COS484 professors—Danqi Chen, Tri Dao, and Vikram Ramaswamy—for their support and encouragement throughout the semester. We are also sincerely thankful to all the TAs for their assistance, especially for providing detailed and constructive feedback on our project proposal, which helped us refine our project direction. Finally, we would like to thank the Department of Computer Science at Princeton for providing the necessary funding to run our experiments via API access.

**Final project for COS484**
